# -*- coding: utf-8 -*-
"""Fake News Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yrgw9UbcF1nT6LHVF5MtQvTnIW0VjMgC

About the Dataset:
   
    1.id: unique for a news article
    2.title: the title of a news article
    3.author:author of the news article
    4.text: the text of the article,could
            be incomplete
    5. label: a label that marks whether the news article is real or fake
      
         1: fake news
         0: real news

Importing the Dependencies
"""

import numpy as np
import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

import nltk
nltk.download('stopwords')

#printing the stopwords
print(stopwords.words('english'))

"""Data PreProcessing"""

#laoding the dataset
news_dataset=pd.read_csv('/content/train.csv' )

news_dataset.shape

news_dataset.head()

#count the no of missing values in the dataset
news_dataset.isnull().sum()

#replacing the null value with empty space
news_dataset=news_dataset.fillna('')

#we don't conside the 'text' column for processing ,as it may be large and will take much time
#that's why we will we use title and author.combine both the column

# merging the author name and news title and store them in new col i.e.; 'content'
news_dataset['content']=news_dataset['author']+ ' ' + news_dataset['title']

print(news_dataset['content'])

#separating the data & label
X=news_dataset.drop(columns='label',axis=1)
Y=news_dataset['label']

print(X)
print(Y)

"""Stemming:
   
   Stemming is the process of reduction a owrd to its Root word

example:
actress,actor,acting----> act
"""

port_stemmer=PorterStemmer()

def stemming(content):
  stemmed_content=re.sub('[^a-zA-Z]',' ',content) #select only character (excluding nos,commas ,etc ,these will replaced by space(' '),we have mentioned)
  stemmed_content=stemmed_content.lower()
  stemmed_content=stemmed_content.split()
  stemmed_content=[port_stemmer.stem(word) for word in stemmed_content if not word in stopwords.words('english')]
  #select all the words except stopwods
  stemmed_content=' '.join(stemmed_content)
  return stemmed_content

#apply this above fun to our content column
news_dataset['content']=news_dataset['content'].apply(stemming)

print(news_dataset['content'])

#separating the data & Label
X=news_dataset['content'].values
Y=news_dataset['label'].values

print(X)

print(Y)

Y.shape

#computer can't understand the text
#so we will convert it into numerical form(feature vector)
vectorizer=TfidfVectorizer()
vectorizer.fit(X)
X=vectorizer.transform(X)

print(X)

"""Splitting the dataset into training & test data"""

X_train,X_test,Y_train,Y_test= train_test_split(X,Y,test_size=0.2,stratify=Y,random_state=2)
#stratify:distributes data in equal propotion of its value, of mentioned column,here Y
#so almost equal no of Os and 1s will selected

"""Training the Model: LogisticRegression"""

model=LogisticRegression()

model.fit(X_train,Y_train)

"""Evaluation

Accuracy Score:

Pedicting or training on train data
"""

#accuracy on training data
X_train_prediction=model.predict(X_train) #it will predict the label(or Y_train)

#comparing the predicted value to original value and finding accuracy

train_data_accuracy=accuracy_score(X_train_prediction,Y_train)

print("Accuracy on training data:",training_data_accuracy)

"""Predicting the label on Test Data"""

#accuracy on test data
X_test_prediction=model.predict(X_test)
test_data_accuracy=accuracy_score(X_test_prediction,Y_test)

print("Accuracy on Test Data",test_data_accuracy)

"""Making a Predictive System"""

X_new = X_test[3]

prediction=model.predict(X_new)
print(prediction)

if(prediction==0):
  print("The News is Real.")

else:
  print("The News is Fake.")

#checking the predicted value with original value of Y_test
print(Y_test[3])

